{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three different models we have to consider for this task:\n",
    "1. The model from task 1 that is trained on the state vectors. This model can be tested in the lunar lander game by running the file: lunar_lander_ml_states_player.py. This will save the results to a csv that will be examined below. Alternatively, you may use the csv file: lunarlander_ml_states_rewards.csv which are the rewards I got when I ran the code.\n",
    "2. The model from task 2 that is trained on the image dataset. This model can be tested in the lunar lander game by running the file: lunar_lander_ml_images_player.py. This will save the results to a csv that will be examined below. Alternatively, you may use the csv file: lunarlander_ml_images_rewards.csv which are the rewards I got when I ran the code.\n",
    "3. The model from task 3. This model was a deepQlearning model and thus has already been tested as part of my task 3 evaluation. The results from that experiment will also be compared below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# States trained model\n",
    "result_array_state_vectors = np.array([])\n",
    "with open(\"lunarlander_ml_states_rewards.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        row[0] = float(row[0])\n",
    "        result_array_state_vectors = np.append(result_array_state_vectors, row[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised learning with state vectors achieves a mean reward of: 217.0581780573467 with standard deviation: 49.11343075739714\n"
     ]
    }
   ],
   "source": [
    "m = np.mean(result_array_state_vectors)\n",
    "s = np.std(result_array_state_vectors)\n",
    "print(\"Supervised learning with state vectors achieves a mean reward of:\", m, \"with standard deviation:\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images trained model\n",
    "result_array_images = np.array([])\n",
    "with open(\"lunarlander_ml_images_rewards.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        row[0] = float(row[0])\n",
    "        result_array_images = np.append(result_array_images, row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised learning with images achieves a mean reward of: -314.3909846382998 with standard deviation: 149.00272316831354\n"
     ]
    }
   ],
   "source": [
    "m2 = np.mean(result_array_images)\n",
    "s2 = np.std(result_array_images)\n",
    "print(\"Supervised learning with images achieves a mean reward of:\", m2, \"with standard deviation:\", s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement learning with state vectors achieves a mean reward of: 208.9124220234545 with standard deviation: 48.34306859394585\n"
     ]
    }
   ],
   "source": [
    "# Taking the reward from the task 3 jupyter notebook with reinforcemnt learning \n",
    "%store -r result_reinforcement_learning\n",
    "m3 = np.mean(result_reinforcement_learning)\n",
    "s3 = np.std(result_reinforcement_learning)\n",
    "print(\"Reinforcement learning with state vectors achieves a mean reward of:\", m3, \"with standard deviation:\", s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State vectors vs Image dataset obtain t-statistic of 47.7856712900519 and thus p-value of 5.846945195076932e-167 . Thus the difference we observe is statistically significant at a 95% confidence level\n",
      "\n",
      "State vectors vs reinforcement learning obtain t-statistic of 1.6674336423101666 and thus p-value of 0.09621480890369682 . Thus the difference we observe is not statistically significant at a 95% confidence level\n",
      "\n",
      "Images data vs reinforcement learning obtain t-statistic of -47.12516406233982 and thus p-value of 6.491117069447761e-165 . Thus the difference we observe is statistically significant at a 95% confidence level\n"
     ]
    }
   ],
   "source": [
    "# Do a t-test to confirm that the difference we observe is statistically significant \n",
    "# State vectors vs images data\n",
    "test = ttest_ind(result_array_state_vectors, result_array_images)\n",
    "if test[1]<0.05:\n",
    "    difference = \"statistically significant\"\n",
    "else:\n",
    "    difference = \"not statistically significant\"\n",
    "print(\"State vectors vs Image dataset obtain t-statistic of\", test[0], \"and thus p-value of\", test[1],\". Thus the difference we observe is\", difference, \"at a 95% confidence level\\n\")\n",
    "\n",
    "# State vectors vs reinforcement learning\n",
    "test = ttest_ind(result_array_state_vectors, result_reinforcement_learning)\n",
    "if test[1]<0.05:\n",
    "    difference = \"statistically significant\"\n",
    "else:\n",
    "    difference = \"not statistically significant\"\n",
    "print(\"State vectors vs reinforcement learning obtain t-statistic of\", test[0], \"and thus p-value of\", test[1],\". Thus the difference we observe is\", difference, \"at a 95% confidence level\\n\")\n",
    "\n",
    "# Images data vs reinforcement learning\n",
    "test = ttest_ind(result_array_images, result_reinforcement_learning)\n",
    "if test[1]<0.05:\n",
    "    difference = \"statistically significant\"\n",
    "else:\n",
    "    difference = \"not statistically significant\"\n",
    "print(\"Images data vs reinforcement learning obtain t-statistic of\", test[0], \"and thus p-value of\", test[1],\". Thus the difference we observe is\", difference, \"at a 95% confidence level\")\n",
    "\n",
    "# All three results are significantly different\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reinforcement Learning</th>\n",
       "      <th>States trained model</th>\n",
       "      <th>Images trained model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Reward</th>\n",
       "      <td>208.912</td>\n",
       "      <td>217.058</td>\n",
       "      <td>-314.391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Reinforcement Learning States trained model Images trained model\n",
       "Reward                208.912              217.058             -314.391"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary\n",
    "performance_ranking = pd.DataFrame(index = [\"Reward\"], columns=[\"Reinforcement Learning\", \"States trained model\", \"Images trained model\"])\n",
    "performance_ranking.loc[\"Reward\", \"Reinforcement Learning\"] = m3\n",
    "performance_ranking.loc[\"Reward\", \"States trained model\"] = m\n",
    "performance_ranking.loc[\"Reward\", \"Images trained model\"] = m2\n",
    "performance_ranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document to describe results of the experiment\n",
    "\n",
    "The first point to make when reporting on the results of the experiment is to awknowledge the ranking based on mean reward. There were three approaches, two supervised models trained on states and images respectively and a third model that was a deepQlearning model. The order of performance based on mean reward over 200 test runs can be observed in the performance table above. Clearly the images trained model has not performed well. The task of learning from pure images would need far more training in order to improve. The reinforcement model and states trained model however have performed extremely well. I applied a pairwise t-test to check if the difference in test scores obtained is statistically significant and found that they are not. Thus I conclude that both models are equally good. \n",
    "\n",
    "Another key component to the assessment of the experiment is the amount of computation required to train each model. There are two points I would make here. Firstly, the computation and time required to train the reinforcement model to reach this level was far more than that of the states trained model. However my second comment is that this computation measurement doesnt account for the computation required to obtain this perfect data set. In fact, one could argue that, as the performance was above human standard, the only way to obtain such a good data set in order to train the states model is to first train a reinforcement model to that level. So although it has been successful here to apply supervised learning this is unlikely to scale well to bigger environments. \n",
    "\n",
    "A final interesting output from the experiment is the route the reinforcement model took to learning the best tactics. The model began picking up the physics of the game very quickly. By episode 20 it could last the full 1000 steps of an episode. The model spent approx 200 episodes in this state getting to grips with the physics and trying to stay flying for as long as possible. Then very suddenly (possibly due to the e greedy) the model stumbled across the fact that attempting to land quickly could lead to a far higher reward and very quickly every episode only lasted around 300 steps with rewards jumping up by over 100. It is very interesting to see the game develop tactis itself due to some randomness in the training phase. This would be something interesting to explore in bigger environments. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
